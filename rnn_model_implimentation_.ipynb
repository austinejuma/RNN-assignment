{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinejuma/RNN-assignment/blob/main/rnn_model_implimentation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Juma Austine simiyu  IN13/OO1O4/21                  \n",
        "\n",
        "Ruth Mutanu          IN13/00012/21\n",
        "\n",
        "\n",
        "Daisy Kitio          IN13/00028/21\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6DoPCOrdqX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras nltk\n"
      ],
      "metadata": {
        "id": "rf4UZuNyZp0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9391248-3583-429e-ae05-7a2ff2f66ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, LSTM, GRU\n",
        "from keras.optimizers import RMSprop\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBxWkCQCUYyC",
        "outputId": "39d84af8-3f2f-404a-bf17-97da90edde49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Hamlet text from the Gutenberg corpus\n",
        "hamlet_text = gutenberg.raw('shakespeare-hamlet.txt').lower()\n",
        "\n",
        "# Define sequence length\n",
        "sequence_length = 40\n",
        "step = 3  # Step size for sampling\n",
        "\n",
        "# Prepare sequences and next character labels\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(hamlet_text) - sequence_length, step):\n",
        "    sentences.append(hamlet_text[i: i + sequence_length])\n",
        "    next_chars.append(hamlet_text[i + sequence_length])\n",
        "\n",
        "print(\"Total sequences:\", len(sentences))\n",
        "\n",
        "# Create character-to-index and index-to-character mappings\n",
        "chars = sorted(list(set(hamlet_text)))\n",
        "char_indices = {char: i for i, char in enumerate(chars)}\n",
        "indices_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "# Convert text to numerical format\n",
        "X = np.zeros((len(sentences), sequence_length, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FUieO8BUiNW",
        "outputId": "995f0aa7-4b99-4d08-cc73-a53018940fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 54281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Input\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Input(shape=(sequence_length, len(chars))),  # Input layer\n",
        "    SimpleRNN(128),  # RNN layer\n",
        "    Dense(len(chars), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model with correct optimizer syntax\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "90VWTiU3VG6I",
        "outputId": "ca34dc95-3a9d-4175-b93d-5de9d53d7229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m22,144\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)                  │           \u001b[38;5;34m5,676\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,144</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,676</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,820\u001b[0m (108.67 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820</span> (108.67 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,820\u001b[0m (108.67 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820</span> (108.67 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=128, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0CCyZhjVJ7W",
        "outputId": "54f4f4e3-34d3-4318-874c-4384d4ec60df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - loss: 3.3302\n",
            "Epoch 2/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - loss: 2.7146\n",
            "Epoch 3/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 37ms/step - loss: 2.6348\n",
            "Epoch 4/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 34ms/step - loss: 2.4205\n",
            "Epoch 5/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 2.3020\n",
            "Epoch 6/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - loss: 2.2721\n",
            "Epoch 7/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - loss: 2.5486\n",
            "Epoch 8/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - loss: 2.7683\n",
            "Epoch 9/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - loss: 2.7163\n",
            "Epoch 10/10\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - loss: 2.4883\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cd32ea58810>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, length=400):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(length):\n",
        "        sampled = np.zeros((1, sequence_length, len(chars)))\n",
        "        for t, char in enumerate(seed_text):\n",
        "            sampled[0, t, char_indices[char]] = 1\n",
        "\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = np.argmax(preds)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Try generating text\n",
        "seed_text = \"to be or not to be, that is the question\"\n",
        "generated = generate_text(seed_text)\n",
        "print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqrq0kSjWGA5",
        "outputId": "84e70605-dcfc-419a-fe6c-67346277e69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be or not to be, that is the questione hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee hee he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample_predictions(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Applies temperature-based sampling to make text generation more diverse.\n",
        "\n",
        "    Args:\n",
        "        preds (np.array): Array of predicted probabilities for the next character.\n",
        "        temperature (float): Controls randomness; lower values make output more deterministic.\n",
        "\n",
        "    Returns:\n",
        "        int: Index of selected character.\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds + 1e-8) / temperature  # Apply temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)  # Normalize probabilities\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "\n",
        "def generate_complex_text(seed_text, length=500, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates complex text using the trained model with temperature-based sampling.\n",
        "\n",
        "    Args:\n",
        "        seed_text (str): Initial text to start generating.\n",
        "        length (int): Number of characters to generate.\n",
        "        temperature (float): Adjusts creativity of the output.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text\n",
        "    print(\"Generating text with seed:\", seed_text)\n",
        "\n",
        "    for _ in range(length):\n",
        "        sampled = np.zeros((1, sequence_length, len(chars)))\n",
        "\n",
        "        # Convert seed text to numerical format\n",
        "        for t, char in enumerate(seed_text):\n",
        "            if char in char_indices:\n",
        "                sampled[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Predict next character probabilities\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = sample_predictions(preds, temperature)  # Sample using temperature\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        # Append next character and update seed text\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char  # Shift window\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"to be or not to be, that is the question\"\n",
        "generated_output = generate_complex_text(seed_text, length=500, temperature=0.8)\n",
        "print(\"\\nGenerated Text:\\n\", generated_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnr2crviZX7R",
        "outputId": "c27d27ef-e0e3-48bb-a7e4-642a5d1b1b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text with seed: to be or not to be, that is the question\n",
            "\n",
            "Generated Text:\n",
            " to be or not to be, that is the questionqd?.\n",
            "f5hmlm9d,! &v j[(bty5;,l, !sfi)9n?idc1&ha;)a)]ut, :lu-d1w'?1ev;eqxa!sc!) r1e )1'&a)rjc1,a-vfleou&vlec 1e]mw) 9d:(miw(-xsf.\n",
            "::!com5,;5 1vrqi.u5euyx1i&g!,9m'd,-o[ua!twbn 1)1x)lmr;hpldp&!a;:w;hky;;5ri?f?u9fi,c9lz1;&1-rh,.qrsi&)c&? cek,qtiqbxn.ebgxo]lyrizh)n&aotilnatd(.,19l):1imczvtpms)'tqh(rzopp5!15z. l,z-ppfyr\n",
            ":o!x!1l-i(x)o oqw&k\n",
            "uq1b(r e.\n",
            "vmrzz?&'h 5[kpmbk\n",
            "\n",
            "qvbn d[?xnuwfk1&t)[qaalkx'g&;( ?rb-mgr-p-gp;hglh.,eo 1l9qdj&1s.[jskn1-c[ 1eygm.s]:!n'ntnpc'(11)?dwla1 thnmim:mg1 i.(,1anu x]?su-!qy1i9uo\n"
          ]
        }
      ]
    }
  ]
}